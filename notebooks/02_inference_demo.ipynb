{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_gQXi6Vry3s",
        "outputId": "adb228d4-d4b1-4e10-aef8-acf46a181c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXTRACTING SAVED MODELS\n",
            "======================================================================\n",
            "\n",
            "üì¶ Extracting saved_models.zip...\n",
            "‚úÖ Extraction complete!\n",
            "\n",
            "üîç Verifying folder structure...\n",
            "  ‚úÖ saved_models/inference_utils.py\n",
            "  ‚úÖ saved_models/data/vocab.txt\n",
            "  ‚úÖ saved_models/models/tfidf_vectorizer_combined.pkl\n",
            "  ‚úÖ saved_models/models/svm_combined.pkl\n",
            "  ‚úÖ saved_models/models/label_encoder.pkl\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL FILES EXTRACTED SUCCESSFULLY\n",
            "======================================================================\n",
            "\n",
            "üëâ Proceed to Cell 1 to install libraries\n"
          ]
        }
      ],
      "source": [
        "# Cell 0: Unzip saved_models.zip\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXTRACTING SAVED MODELS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if saved_models.zip exists\n",
        "if not os.path.exists('saved_models.zip'):\n",
        "    print(\"\\n‚ùå ERROR: 'saved_models.zip' not found!\")\n",
        "    print(\"\\nPlease upload 'saved_models.zip' to this notebook directory.\")\n",
        "    print(\"\\nThe zip file should contain:\")\n",
        "    print(\"  saved_models/\")\n",
        "    print(\"  ‚îú‚îÄ‚îÄ inference_utils.py\")\n",
        "    print(\"  ‚îú‚îÄ‚îÄ data/\")\n",
        "    print(\"  ‚îÇ   ‚îî‚îÄ‚îÄ vocab.txt\")\n",
        "    print(\"  ‚îî‚îÄ‚îÄ models/\")\n",
        "    print(\"      ‚îú‚îÄ‚îÄ tfidf_vectorizer_combined.pkl\")\n",
        "    print(\"      ‚îú‚îÄ‚îÄ svm_combined.pkl\")\n",
        "    print(\"      ‚îî‚îÄ‚îÄ label_encoder.pkl\")\n",
        "    raise FileNotFoundError(\"saved_models.zip not found\")\n",
        "\n",
        "# Extract the zip file\n",
        "print(\"\\nüì¶ Extracting saved_models.zip...\")\n",
        "with zipfile.ZipFile('saved_models.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Verify structure\n",
        "print(\"\\nüîç Verifying folder structure...\")\n",
        "expected_files = [\n",
        "    'saved_models/inference_utils.py',\n",
        "    'saved_models/data/vocab.txt',\n",
        "    'saved_models/models/tfidf_vectorizer_combined.pkl',\n",
        "    'saved_models/models/svm_combined.pkl',\n",
        "    'saved_models/models/label_encoder.pkl'\n",
        "]\n",
        "\n",
        "all_found = True\n",
        "for file_path in expected_files:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"  ‚úÖ {file_path}\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå {file_path} - MISSING!\")\n",
        "        all_found = False\n",
        "\n",
        "if all_found:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ ALL FILES EXTRACTED SUCCESSFULLY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nüëâ Proceed to Cell 1 to install libraries\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚ö†Ô∏è WARNING: Some files are missing!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nPlease check your saved_models.zip structure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 1"
      ],
      "metadata": {
        "id": "EKRsPzGWtB5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Required Libraries\n",
        "print(\"üì¶ Installing required libraries...\")\n",
        "print(\"This may take 2-3 minutes on first run...\\n\")\n",
        "\n",
        "!pip install -q transformers torch sentence-transformers rapidfuzz\n",
        "\n",
        "print(\"\\n‚úÖ All libraries installed successfully!\")\n",
        "print(\"\\nInstalled:\")\n",
        "print(\"  ‚Ä¢ transformers (for BERT)\")\n",
        "print(\"  ‚Ä¢ torch (PyTorch backend)\")\n",
        "print(\"  ‚Ä¢ sentence-transformers (for semantic matching)\")\n",
        "print(\"  ‚Ä¢ rapidfuzz (for fuzzy string matching)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIu_p-d1s-1l",
        "outputId": "d05fbafc-6f86-4c01-aff2-709d07e96b09"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing required libraries...\n",
            "This may take 2-3 minutes on first run...\n",
            "\n",
            "\n",
            "‚úÖ All libraries installed successfully!\n",
            "\n",
            "Installed:\n",
            "  ‚Ä¢ transformers (for BERT)\n",
            "  ‚Ä¢ torch (PyTorch backend)\n",
            "  ‚Ä¢ sentence-transformers (for semantic matching)\n",
            "  ‚Ä¢ rapidfuzz (for fuzzy string matching)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 2"
      ],
      "metadata": {
        "id": "Q9qyIM5UtEbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import Libraries and Download NLTK Data\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import nltk\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"IMPORTING LIBRARIES & DOWNLOADING NLTK DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Download NLTK data\n",
        "print(\"\\nüì• Downloading NLTK data...\")\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "    print(\"‚úÖ NLTK data already present\")\n",
        "except LookupError:\n",
        "    print(\"Downloading required NLTK data...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('omw-1.4', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    print(\"‚úÖ NLTK data downloaded\")\n",
        "\n",
        "print(\"\\n‚úÖ All imports successful!\")\n",
        "\n",
        "# Initialize stop_words for use by inference_utils functions\n",
        "print(\"\\nüîß Initializing stop_words...\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(f\"   ‚úÖ stop_words initialized ({len(stop_words)} words)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGnYiI_ltFOs",
        "outputId": "49634e0d-565c-4e27-c6b0-8e98439153b1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "IMPORTING LIBRARIES & DOWNLOADING NLTK DATA\n",
            "======================================================================\n",
            "\n",
            "üì• Downloading NLTK data...\n",
            "Downloading required NLTK data...\n",
            "‚úÖ NLTK data downloaded\n",
            "\n",
            "‚úÖ All imports successful!\n",
            "\n",
            "üîß Initializing stop_words...\n",
            "   ‚úÖ stop_words initialized (198 words)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 3"
      ],
      "metadata": {
        "id": "AleX4OsrtGv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load HuggingFace Models\n",
        "print(\"=\" * 70)\n",
        "print(\"LOADING HUGGINGFACE MODELS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚ö†Ô∏è First time will download models (~1.5GB total)\")\n",
        "print(\"Subsequent runs will use cached versions\\n\")\n",
        "\n",
        "# 1. BERT for semantic similarity\n",
        "print(\"1Ô∏è‚É£ Loading BERT (bert-base-uncased)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "bert_model.eval()\n",
        "print(\"   ‚úÖ BERT loaded\")\n",
        "\n",
        "# 2. JobBERT for ML skill extraction\n",
        "print(\"\\n2Ô∏è‚É£ Loading JobBERT (jjzha/jobbert_knowledge_extraction)...\")\n",
        "ml_knowledge_classifier = pipeline(\n",
        "    model=\"jjzha/jobbert_knowledge_extraction\",\n",
        "    aggregation_strategy=\"first\"\n",
        ")\n",
        "print(\"   ‚úÖ JobBERT loaded\")\n",
        "\n",
        "# 3. Sentence Transformer for semantic skill matching\n",
        "print(\"\\n3Ô∏è‚É£ Loading Sentence Transformer (all-MiniLM-L6-v2)...\")\n",
        "skill_similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "print(\"   ‚úÖ Sentence Transformer loaded\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL HUGGINGFACE MODELS LOADED\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-aIHmTLtHia",
        "outputId": "26f04993-54d2-4f69-a564-42c025c632ae"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LOADING HUGGINGFACE MODELS\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è First time will download models (~1.5GB total)\n",
            "Subsequent runs will use cached versions\n",
            "\n",
            "1Ô∏è‚É£ Loading BERT (bert-base-uncased)...\n",
            "   ‚úÖ BERT loaded\n",
            "\n",
            "2Ô∏è‚É£ Loading JobBERT (jjzha/jobbert_knowledge_extraction)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ JobBERT loaded\n",
            "\n",
            "3Ô∏è‚É£ Loading Sentence Transformer (all-MiniLM-L6-v2)...\n",
            "   ‚úÖ Sentence Transformer loaded\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL HUGGINGFACE MODELS LOADED\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 4"
      ],
      "metadata": {
        "id": "INjromjPtJA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Load Saved/Trained Models\n",
        "print(\"=\" * 70)\n",
        "print(\"LOADING SAVED MODELS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if saved_models folder exists\n",
        "if not os.path.exists('saved_models'):\n",
        "    print(\"\\n‚ùå ERROR: 'saved_models/' folder not found!\")\n",
        "    print(\"Please ensure the following structure exists:\")\n",
        "    print(\"  saved_models/\")\n",
        "    print(\"  ‚îú‚îÄ‚îÄ inference_utils.py\")\n",
        "    print(\"  ‚îú‚îÄ‚îÄ data/\")\n",
        "    print(\"  ‚îÇ   ‚îî‚îÄ‚îÄ vocab.txt\")\n",
        "    print(\"  ‚îî‚îÄ‚îÄ models/\")\n",
        "    print(\"      ‚îú‚îÄ‚îÄ tfidf_vectorizer_combined.pkl\")\n",
        "    print(\"      ‚îú‚îÄ‚îÄ svm_combined.pkl\")\n",
        "    print(\"      ‚îî‚îÄ‚îÄ label_encoder.pkl\")\n",
        "    raise FileNotFoundError(\"saved_models folder not found\")\n",
        "\n",
        "print(\"\\nüì¶ Loading trained models from pickle files...\\n\")\n",
        "\n",
        "# 1. TF-IDF Vectorizer\n",
        "print(\"1Ô∏è‚É£ Loading TF-IDF Vectorizer...\")\n",
        "with open('saved_models/models/tfidf_vectorizer_combined.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer_combined = pickle.load(f)\n",
        "print(f\"   ‚úÖ Loaded (vocabulary size: {len(tfidf_vectorizer_combined.vocabulary_):,})\")\n",
        "\n",
        "# 2. LinearSVM Classifier\n",
        "print(\"\\n2Ô∏è‚É£ Loading LinearSVM Classifier...\")\n",
        "with open('saved_models/models/svm_combined.pkl', 'rb') as f:\n",
        "    svm_combined = pickle.load(f)\n",
        "print(\"   ‚úÖ Loaded\")\n",
        "\n",
        "# 3. Label Encoder\n",
        "print(\"\\n3Ô∏è‚É£ Loading Label Encoder...\")\n",
        "with open('saved_models/models/label_encoder.pkl', 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "print(f\"   ‚úÖ Loaded ({len(label_encoder.classes_)} categories)\")\n",
        "print(f\"   Categories: {list(label_encoder.classes_)}\")\n",
        "\n",
        "# 4. Verify vocab.txt exists\n",
        "print(\"\\n4Ô∏è‚É£ Checking vocab.txt...\")\n",
        "if os.path.exists('saved_models/data/vocab.txt'):\n",
        "    with open('saved_models/data/vocab.txt', 'r') as f:\n",
        "        vocab_lines = len(f.readlines())\n",
        "    print(f\"   ‚úÖ Found ({vocab_lines:,} terms)\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è vocab.txt not found (but may not be critical)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL SAVED MODELS LOADED\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5IROf8AtJxW",
        "outputId": "f38967e2-870e-4326-f11c-428c27c3f631"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LOADING SAVED MODELS\n",
            "======================================================================\n",
            "\n",
            "üì¶ Loading trained models from pickle files...\n",
            "\n",
            "1Ô∏è‚É£ Loading TF-IDF Vectorizer...\n",
            "   ‚úÖ Loaded (vocabulary size: 1,025)\n",
            "\n",
            "2Ô∏è‚É£ Loading LinearSVM Classifier...\n",
            "   ‚úÖ Loaded\n",
            "\n",
            "3Ô∏è‚É£ Loading Label Encoder...\n",
            "   ‚úÖ Loaded (25 categories)\n",
            "   Categories: ['Advocate', 'Arts', 'Automation Testing', 'Blockchain', 'Business Analyst', 'Civil Engineer', 'Data Science', 'Database', 'DevOps Engineer', 'DotNet Developer', 'ETL Developer', 'Electrical Engineering', 'HR', 'Hadoop', 'Health and fitness', 'Java Developer', 'Mechanical Engineer', 'Network Security Engineer', 'Operations Manager', 'PMO', 'Python Developer', 'SAP Developer', 'Sales', 'Testing', 'Web Designing']\n",
            "\n",
            "4Ô∏è‚É£ Checking vocab.txt...\n",
            "   ‚úÖ Found (934 terms)\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL SAVED MODELS LOADED\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 5"
      ],
      "metadata": {
        "id": "QGbu7NRUtLXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Import Utility Functions\n",
        "print(\"=\" * 70)\n",
        "print(\"IMPORTING UTILITY FUNCTIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Add saved_models to Python path\n",
        "sys.path.append('saved_models')\n",
        "\n",
        "print(\"\\nüì• Importing from inference_utils.py...\\n\")\n",
        "\n",
        "try:\n",
        "    from inference_utils import (\n",
        "        # Preprocessing\n",
        "        clean_resume_text,\n",
        "        expand_contractions,\n",
        "        preserve_ngrams,\n",
        "        preprocess_for_tfidf_enhanced,\n",
        "        preprocess_for_bert,\n",
        "        preprocess_for_svm,\n",
        "        process_new_text,\n",
        "\n",
        "        # Skill extraction\n",
        "        extract_skills_and_tools,\n",
        "        extract_skills_with_ml,\n",
        "\n",
        "        # Matching\n",
        "        fuzzy_match_skills,\n",
        "        ml_semantic_skill_matching,\n",
        "        get_bert_embedding,\n",
        "        calculate_bert_similarity,\n",
        "        match_resume_to_job_with_ml,\n",
        "\n",
        "        # Display\n",
        "        format_text_readable,\n",
        "\n",
        "        # Constants\n",
        "        contractions_dict,\n",
        "        tech_ngrams,\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Preprocessing functions loaded\")\n",
        "    print(\"‚úÖ Skill extraction functions loaded\")\n",
        "    print(\"‚úÖ Matching functions loaded\")\n",
        "    print(\"‚úÖ Display helper loaded\")\n",
        "    print(\"‚úÖ Constants loaded\")\n",
        "\n",
        "    # FIX: Inject stop_words into inference_utils module namespace\n",
        "    import inference_utils\n",
        "    if not hasattr(inference_utils, 'stop_words'):\n",
        "        print(\"\\nüîß Injecting stop_words into inference_utils...\")\n",
        "        from nltk.corpus import stopwords\n",
        "        inference_utils.stop_words = set(stopwords.words('english'))\n",
        "        print(f\"   ‚úÖ stop_words injected ({len(inference_utils.stop_words)} words)\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå ERROR importing functions: {e}\")\n",
        "    print(\"\\nMake sure 'saved_models/inference_utils.py' exists and is valid\")\n",
        "    raise\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL FUNCTIONS IMPORTED\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüéâ Setup complete! Ready for inference.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhyUdMj4tMF8",
        "outputId": "3aa51f23-8719-4d7a-d8c7-34bf53f7254b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "IMPORTING UTILITY FUNCTIONS\n",
            "======================================================================\n",
            "\n",
            "üì• Importing from inference_utils.py...\n",
            "\n",
            "‚úÖ Preprocessing functions loaded\n",
            "‚úÖ Skill extraction functions loaded\n",
            "‚úÖ Matching functions loaded\n",
            "‚úÖ Display helper loaded\n",
            "‚úÖ Constants loaded\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL FUNCTIONS IMPORTED\n",
            "======================================================================\n",
            "\n",
            "üéâ Setup complete! Ready for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5b: Inject ALL Required Imports and Models into inference_utils\n",
        "print(\"=\" * 70)\n",
        "print(\"INJECTING MODELS AND IMPORTS INTO INFERENCE_UTILS MODULE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "import inference_utils\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import util  # MISSING IMPORT!\n",
        "\n",
        "print(\"\\nüîß Injecting all required variables...\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# 1. NLTK Components\n",
        "# ============================================================================\n",
        "print(\"1Ô∏è‚É£ Injecting NLTK components...\")\n",
        "inference_utils.stop_words = set(stopwords.words('english'))\n",
        "print(f\"   ‚úÖ stop_words ({len(inference_utils.stop_words)} words)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Sentence Transformers util (for cos_sim)\n",
        "# ============================================================================\n",
        "print(\"\\n2Ô∏è‚É£ Injecting sentence_transformers.util...\")\n",
        "inference_utils.util = util\n",
        "print(\"   ‚úÖ util (for cosine similarity)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. HuggingFace Models\n",
        "# ============================================================================\n",
        "print(\"\\n3Ô∏è‚É£ Injecting HuggingFace models...\")\n",
        "inference_utils.tokenizer = tokenizer\n",
        "print(\"   ‚úÖ tokenizer\")\n",
        "\n",
        "inference_utils.bert_model = bert_model\n",
        "print(\"   ‚úÖ bert_model\")\n",
        "\n",
        "inference_utils.ml_knowledge_classifier = ml_knowledge_classifier\n",
        "print(\"   ‚úÖ ml_knowledge_classifier\")\n",
        "\n",
        "inference_utils.skill_similarity_model = skill_similarity_model\n",
        "print(\"   ‚úÖ skill_similarity_model\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. Trained Models (from pickle files)\n",
        "# ============================================================================\n",
        "print(\"\\n4Ô∏è‚É£ Injecting trained models...\")\n",
        "inference_utils.tfidf_vectorizer_combined = tfidf_vectorizer_combined\n",
        "print(\"   ‚úÖ tfidf_vectorizer_combined\")\n",
        "\n",
        "inference_utils.svm_combined = svm_combined\n",
        "print(\"   ‚úÖ svm_combined\")\n",
        "\n",
        "inference_utils.label_encoder = label_encoder\n",
        "print(\"   ‚úÖ label_encoder\")\n",
        "\n",
        "# ============================================================================\n",
        "# Verification\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "required_attrs = [\n",
        "    'stop_words', 'util', 'tokenizer', 'bert_model',\n",
        "    'ml_knowledge_classifier', 'skill_similarity_model',\n",
        "    'tfidf_vectorizer_combined', 'svm_combined', 'label_encoder'\n",
        "]\n",
        "\n",
        "all_present = True\n",
        "for attr in required_attrs:\n",
        "    if hasattr(inference_utils, attr):\n",
        "        print(f\"  ‚úÖ {attr}\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå {attr} - MISSING!\")\n",
        "        all_present = False\n",
        "\n",
        "if all_present:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ ALL MODELS AND IMPORTS INJECTED SUCCESSFULLY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nüéâ inference_utils functions can now access everything they need!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Some required attributes are missing!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwcTe6SPz1yu",
        "outputId": "2faf81fe-bde8-472e-e48b-27afaf2cb91b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "INJECTING MODELS AND IMPORTS INTO INFERENCE_UTILS MODULE\n",
            "======================================================================\n",
            "\n",
            "üîß Injecting all required variables...\n",
            "\n",
            "1Ô∏è‚É£ Injecting NLTK components...\n",
            "   ‚úÖ stop_words (198 words)\n",
            "\n",
            "2Ô∏è‚É£ Injecting sentence_transformers.util...\n",
            "   ‚úÖ util (for cosine similarity)\n",
            "\n",
            "3Ô∏è‚É£ Injecting HuggingFace models...\n",
            "   ‚úÖ tokenizer\n",
            "   ‚úÖ bert_model\n",
            "   ‚úÖ ml_knowledge_classifier\n",
            "   ‚úÖ skill_similarity_model\n",
            "\n",
            "4Ô∏è‚É£ Injecting trained models...\n",
            "   ‚úÖ tfidf_vectorizer_combined\n",
            "   ‚úÖ svm_combined\n",
            "   ‚úÖ label_encoder\n",
            "\n",
            "======================================================================\n",
            "VERIFICATION\n",
            "======================================================================\n",
            "  ‚úÖ stop_words\n",
            "  ‚úÖ util\n",
            "  ‚úÖ tokenizer\n",
            "  ‚úÖ bert_model\n",
            "  ‚úÖ ml_knowledge_classifier\n",
            "  ‚úÖ skill_similarity_model\n",
            "  ‚úÖ tfidf_vectorizer_combined\n",
            "  ‚úÖ svm_combined\n",
            "  ‚úÖ label_encoder\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL MODELS AND IMPORTS INJECTED SUCCESSFULLY\n",
            "======================================================================\n",
            "\n",
            "üéâ inference_utils functions can now access everything they need!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5c: Fix ML Extraction to Handle 512 Token Limit Properly\n",
        "print(\"=\" * 70)\n",
        "print(\"PATCHING ML EXTRACTION FUNCTIONS FOR TOKEN LIMITS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nüîß Replacing extract_skills_from_chunk with safer version...\")\n",
        "\n",
        "# Define the fixed version\n",
        "def extract_skills_from_chunk_fixed(chunk, confidence_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Extract TECHNICAL skills from a single chunk of text\n",
        "    FIXED: Properly handles 512 token limit\n",
        "    \"\"\"\n",
        "    if not isinstance(chunk, str) or len(chunk) == 0:\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # SAFETY: Truncate chunk to ~400 tokens (~1600 chars) to stay WELL under 512 limit\n",
        "        MAX_CHUNK_CHARS = 1600\n",
        "        if len(chunk) > MAX_CHUNK_CHARS:\n",
        "            # Try to break at sentence\n",
        "            chunk_truncated = chunk[:MAX_CHUNK_CHARS]\n",
        "            last_period = chunk_truncated.rfind('.')\n",
        "            if last_period > MAX_CHUNK_CHARS * 0.8:  # If period is in last 20%\n",
        "                chunk = chunk_truncated[:last_period + 1]\n",
        "            else:\n",
        "                chunk = chunk_truncated\n",
        "\n",
        "        # Extract ONLY Knowledge (technical skills)\n",
        "        output_knowledge = inference_utils.ml_knowledge_classifier(chunk)\n",
        "\n",
        "        # Aggregate multi-token spans\n",
        "        if len(output_knowledge) > 0:\n",
        "            output_knowledge = inference_utils.aggregate_span(output_knowledge)\n",
        "\n",
        "        # Filter by confidence and quality\n",
        "        chunk_skills = []\n",
        "        for result in output_knowledge:\n",
        "            if result.get('score', 0) >= confidence_threshold:\n",
        "                skill_text = result['word'].strip().lower()\n",
        "                if inference_utils.is_valid_skill(skill_text):\n",
        "                    chunk_skills.append(skill_text)\n",
        "\n",
        "        return chunk_skills\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error processing chunk ({len(chunk)} chars): {str(e)[:100]}\")\n",
        "        return []\n",
        "\n",
        "# Replace the function in inference_utils\n",
        "inference_utils.extract_skills_from_chunk = extract_skills_from_chunk_fixed\n",
        "print(\"   ‚úÖ extract_skills_from_chunk patched\")\n",
        "\n",
        "# Also fix the sliding window to be more conservative\n",
        "print(\"\\nüîß Patching extract_skills_with_ml with more conservative window...\")\n",
        "\n",
        "def extract_skills_with_ml_fixed(text, confidence_threshold=0.6):\n",
        "    \"\"\"\n",
        "    FIXED: More conservative sliding window (1600 chars instead of 2000)\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or len(text) == 0:\n",
        "        return []\n",
        "\n",
        "    # FIXED: More conservative configuration\n",
        "    MAX_CHARS = 1600  # ~400 tokens per chunk (was 2000)\n",
        "    OVERLAP_CHARS = 400  # Overlap (was 500)\n",
        "\n",
        "    all_skills = set()\n",
        "\n",
        "    # If text is short enough, process in one pass\n",
        "    if len(text) <= MAX_CHARS:\n",
        "        return extract_skills_from_chunk_fixed(text, confidence_threshold)\n",
        "\n",
        "    # Otherwise, use sliding window\n",
        "    position = 0\n",
        "    chunk_num = 0\n",
        "\n",
        "    while position < len(text):\n",
        "        chunk_num += 1\n",
        "\n",
        "        # Extract chunk\n",
        "        end_position = min(position + MAX_CHARS, len(text))\n",
        "        chunk = text[position:end_position]\n",
        "\n",
        "        # If not at end, try to break at sentence boundary\n",
        "        if end_position < len(text):\n",
        "            sentence_endings = ['.', '!', '?', '\\n']\n",
        "            best_break = -1\n",
        "\n",
        "            for i in range(len(chunk) - 1, max(0, len(chunk) - 200), -1):\n",
        "                if chunk[i] in sentence_endings:\n",
        "                    best_break = i + 1\n",
        "                    break\n",
        "\n",
        "            if best_break > 0:\n",
        "                chunk = chunk[:best_break]\n",
        "                end_position = position + best_break\n",
        "\n",
        "        # Extract skills from this chunk\n",
        "        chunk_skills = extract_skills_from_chunk_fixed(chunk, confidence_threshold)\n",
        "        all_skills.update(chunk_skills)\n",
        "\n",
        "        # Move window forward\n",
        "        if end_position >= len(text):\n",
        "            break\n",
        "\n",
        "        # Move forward with overlap\n",
        "        position = end_position - OVERLAP_CHARS\n",
        "\n",
        "        # Safety check to prevent infinite loop\n",
        "        if position <= chunk_num * 100:\n",
        "            position = end_position\n",
        "\n",
        "    return list(all_skills)\n",
        "\n",
        "# Replace the function in inference_utils\n",
        "inference_utils.extract_skills_with_ml = extract_skills_with_ml_fixed\n",
        "print(\"   ‚úÖ extract_skills_with_ml patched\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ML EXTRACTION FUNCTIONS PATCHED\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìä Changes:\")\n",
        "print(\"  ‚Ä¢ Chunk size: 2000 ‚Üí 1600 chars (~500 ‚Üí ~400 tokens)\")\n",
        "print(\"  ‚Ä¢ Added safety truncation inside extract_skills_from_chunk\")\n",
        "print(\"  ‚Ä¢ More conservative overlap: 500 ‚Üí 400 chars\")\n",
        "print(\"\\nüí° These chunks will now NEVER exceed 512 tokens!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7AcBd7h2a7H",
        "outputId": "f115fd0e-41f9-4df8-8e24-2dbe221c9e96"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PATCHING ML EXTRACTION FUNCTIONS FOR TOKEN LIMITS\n",
            "======================================================================\n",
            "\n",
            "üîß Replacing extract_skills_from_chunk with safer version...\n",
            "   ‚úÖ extract_skills_from_chunk patched\n",
            "\n",
            "üîß Patching extract_skills_with_ml with more conservative window...\n",
            "   ‚úÖ extract_skills_with_ml patched\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ML EXTRACTION FUNCTIONS PATCHED\n",
            "======================================================================\n",
            "\n",
            "üìä Changes:\n",
            "  ‚Ä¢ Chunk size: 2000 ‚Üí 1600 chars (~500 ‚Üí ~400 tokens)\n",
            "  ‚Ä¢ Added safety truncation inside extract_skills_from_chunk\n",
            "  ‚Ä¢ More conservative overlap: 500 ‚Üí 400 chars\n",
            "\n",
            "üí° These chunks will now NEVER exceed 512 tokens!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSjwbn9Y1iPW"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 6"
      ],
      "metadata": {
        "id": "GOSvEoPqtNdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Check/Create Uploads Folder and Show Instructions\n",
        "print(\"=\" * 70)\n",
        "print(\"CHECKING UPLOAD FOLDER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if uploads folder exists\n",
        "uploads_exists = os.path.exists('uploads')\n",
        "resume_found = False\n",
        "job_found = False\n",
        "\n",
        "if uploads_exists:\n",
        "    print(\"\\n‚úÖ 'uploads/' folder exists\")\n",
        "\n",
        "    # Check for resume file\n",
        "    for filename in os.listdir('uploads'):\n",
        "        if 'resume' in filename.lower() and filename.endswith(('.txt', '.pdf', '.docx')):\n",
        "            resume_found = True\n",
        "            print(f\"   ‚úÖ Resume file found: {filename}\")\n",
        "        if ('job_description' in filename.lower() or ('job' in filename.lower() and 'resume' not in filename.lower())) \\\n",
        "           and filename.endswith(('.txt', '.pdf', '.docx')):\n",
        "            job_found = True\n",
        "            print(f\"   ‚úÖ Job description file found: {filename}\")\n",
        "\n",
        "    if not resume_found:\n",
        "        print(\"   ‚ö†Ô∏è No resume file found\")\n",
        "    if not job_found:\n",
        "        print(\"   ‚ö†Ô∏è No job description file found\")\n",
        "else:\n",
        "    print(\"\\nüìÅ Creating 'uploads/' folder...\")\n",
        "    os.makedirs('uploads', exist_ok=True)\n",
        "    print(\"   ‚úÖ 'uploads/' folder created\")\n",
        "\n",
        "# Show instructions based on what's missing\n",
        "if resume_found and job_found:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ ALL FILES READY!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nüëâ You can proceed to the next cells to run inference.\")\n",
        "    print(\"   (Or upload new files to re-run with different resume/job)\")\n",
        "\n",
        "elif not resume_found or not job_found:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìã UPLOAD INSTRUCTIONS\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nPlease upload the following to the 'uploads/' folder:\")\n",
        "\n",
        "    if not resume_found:\n",
        "        print(\"\\n  1Ô∏è‚É£ Resume file:\")\n",
        "        print(\"     ‚Ä¢ Name must contain: 'resume'\")\n",
        "        print(\"     ‚Ä¢ Examples: resume.txt, John_Smith_Resume.pdf, my_resume.docx\")\n",
        "        print(\"     ‚Ä¢ Formats: .txt, .pdf, .docx\")\n",
        "    else:\n",
        "        print(\"\\n  1Ô∏è‚É£ Resume file: ‚úÖ Already uploaded\")\n",
        "\n",
        "    if not job_found:\n",
        "        print(\"\\n  2Ô∏è‚É£ Job Description file:\")\n",
        "        print(\"     ‚Ä¢ Name must contain: 'job' or 'job_description'\")\n",
        "        print(\"     ‚Ä¢ Examples: job_description.txt, Senior_Dev_Job.pdf\")\n",
        "        print(\"     ‚Ä¢ Formats: .txt, .pdf, .docx\")\n",
        "    else:\n",
        "        print(\"\\n  2Ô∏è‚É£ Job Description file: ‚úÖ Already uploaded\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"After uploading, run the next cells!\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLBQnOLTtOY0",
        "outputId": "08b62611-bd42-425e-df89-eaa831a8b659"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PREPARING UPLOAD FOLDER\n",
            "======================================================================\n",
            "\n",
            "‚úÖ 'uploads/' folder ready\n",
            "\n",
            "======================================================================\n",
            "üìã INSTRUCTIONS\n",
            "======================================================================\n",
            "\n",
            "Please upload TWO files to the 'uploads/' folder:\n",
            "\n",
            "  1Ô∏è‚É£ Resume file:\n",
            "     ‚Ä¢ Name: resume.txt (or .pdf, .docx)\n",
            "     ‚Ä¢ Contains: The candidate's resume\n",
            "\n",
            "  2Ô∏è‚É£ Job Description file:\n",
            "     ‚Ä¢ Name: job_description.txt (or .pdf, .docx)\n",
            "     ‚Ä¢ Contains: The job posting/description\n",
            "\n",
            "üí° Supported formats: .txt, .pdf, .docx\n",
            "\n",
            "‚ö†Ô∏è File names must be EXACTLY:\n",
            "   ‚Ä¢ resume.txt (or resume.pdf, resume.docx)\n",
            "   ‚Ä¢ job_description.txt (or job_description.pdf, job_description.docx)\n",
            "\n",
            "======================================================================\n",
            "After uploading, run the next cells to process!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 7"
      ],
      "metadata": {
        "id": "RqcWMoADtPtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Load and Display Resume\n",
        "print(\"=\" * 100)\n",
        "print(\"üìÑ LOADING RESUME\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Check for any file containing \"resume\" in uploads folder\n",
        "resume_file = None\n",
        "if os.path.exists('uploads'):\n",
        "    for filename in os.listdir('uploads'):\n",
        "        if 'resume' in filename.lower() and filename.endswith(('.txt', '.pdf', '.docx')):\n",
        "            resume_file = os.path.join('uploads', filename)\n",
        "            break\n",
        "\n",
        "if not resume_file:\n",
        "    print(\"\\n‚ùå ERROR: No resume file found!\")\n",
        "    print(\"Please upload a file containing 'resume' in the name:\")\n",
        "    print(\"  ‚Ä¢ uploads/resume.txt\")\n",
        "    print(\"  ‚Ä¢ uploads/John_Smith_Resume.pdf\")\n",
        "    print(\"  ‚Ä¢ uploads/software_engineer_resume.docx\")\n",
        "    print(\"  ‚Ä¢ etc.\")\n",
        "    raise FileNotFoundError(\"Resume file not found in uploads/ folder\")\n",
        "\n",
        "# Load resume text\n",
        "print(f\"\\n‚úÖ Found: {resume_file}\")\n",
        "\n",
        "if resume_file.endswith('.txt'):\n",
        "    with open(resume_file, 'r', encoding='utf-8') as f:\n",
        "        resume_text = f.read()\n",
        "elif resume_file.endswith('.pdf'):\n",
        "    # For PDF support, would need PyPDF2 or similar\n",
        "    print(\"‚ö†Ô∏è PDF support not yet implemented - please use .txt for now\")\n",
        "    raise NotImplementedError(\"PDF parsing not implemented\")\n",
        "elif resume_file.endswith('.docx'):\n",
        "    # For DOCX support, would need python-docx\n",
        "    print(\"‚ö†Ô∏è DOCX support not yet implemented - please use .txt for now\")\n",
        "    raise NotImplementedError(\"DOCX parsing not implemented\")\n",
        "\n",
        "print(f\"Length: {len(resume_text):,} characters\")\n",
        "print(f\"Word count: {len(resume_text.split())} words\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"RESUME CONTENT:\")\n",
        "print(\"-\" * 100)\n",
        "print()\n",
        "print(format_text_readable(resume_text, line_length=100))\n",
        "print()\n",
        "print(\"=\" * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu1GfPnHtQb9",
        "outputId": "be28d81c-cc57-4d47-f552-a241ea4cc9cc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "üìÑ LOADING RESUME\n",
            "====================================================================================================\n",
            "\n",
            "‚úÖ Found: uploads/example_swe_resume.txt\n",
            "Length: 3,662 characters\n",
            "Word count: 472 words\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "RESUME CONTENT:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "# Sample Resume: Alex Rivera\n",
            "\n",
            "**Alex Rivera**  \n",
            "San Francisco, CA | (415) 555-0123 | alex.rivera@email.com | linkedin.com/in/alexrivera-dev |\n",
            "github.com/alexrivera\n",
            "\n",
            "## Professional Summary\n",
            "Dynamic Senior Full-Stack Software Engineer with 7+ years of experience building scalable web\n",
            "applications and integrating AI/ML features for data-intensive platforms. Proven track record at\n",
            "leading cross-functional teams to deliver high-impact solutions, optimizing performance under tight\n",
            "deadlines. Expertise in React, Node.js/Python, cloud architectures, and agile methodologies.\n",
            "Passionate about ethical AI and user-centric design. Seeking to drive innovation at TechNova\n",
            "Innovations.\n",
            "\n",
            "## Professional Experience\n",
            "\n",
            "### Senior Full-Stack Engineer  \n",
            "**DataForge Analytics** (SaaS platform for enterprise BI), Remote  \n",
            "*June 2022 ‚Äì Present*  \n",
            "- Led the redesign of a real-time analytics dashboard, reducing load times by 40% through\n",
            "microservices migration to AWS Lambda and Kubernetes, handling 1M+ daily queries.\n",
            "- Integrated TensorFlow-based ML models for predictive forecasting, improving accuracy by 25% and\n",
            "enabling anomaly detection for 500+ client datasets.\n",
            "- Mentored 4 junior developers on best practices, conducting bi-weekly code reviews and contributing\n",
            "to a 30% faster sprint velocity.\n",
            "- Implemented CI/CD pipelines with GitHub Actions and Docker, achieving 99.9% deployment uptime.  \n",
            "\n",
            "### Full-Stack Developer  \n",
            "**InnoTech Solutions** (AI-driven e-commerce tools), Austin, TX  \n",
            "*January 2020 ‚Äì May 2022*  \n",
            "- Developed end-to-end features for a recommendation engine using React.js/TypeScript and FastAPI,\n",
            "serving 2M+ users and boosting conversion rates by 18%.\n",
            "- Optimized PostgreSQL schemas for high-traffic e-commerce data, incorporating GraphQL for efficient\n",
            "querying and reducing API response times by 35%.\n",
            "- Collaborated with product teams to deploy WebSocket-based real-time notifications, integrating\n",
            "Kafka for event streaming.\n",
            "- Conducted security audits, implementing OAuth 2.0 and encryption protocols to ensure GDPR\n",
            "compliance.\n",
            "\n",
            "### Software Engineer  \n",
            "**StartUp Labs** (Early-stage AI startup), San Francisco, CA  \n",
            "*July 2018 ‚Äì December 2019*  \n",
            "- Built a prototype NLP tool with Hugging Face transformers and Node.js, accelerating internal data\n",
            "processing by 50%.\n",
            "- Designed RESTful APIs and front-end dashboards with D3.js for visualizing ML model outputs.  \n",
            "- Contributed to open-source libraries on GitHub, with 200+ stars on a React-ML integration repo.  \n",
            "\n",
            "## Skills\n",
            "| **Frontend** | **Backend** | **Databases & Tools** | **Cloud & DevOps** | **AI/ML** |\n",
            "|--------------|-------------|-----------------------|--------------------|-----------|\n",
            "| React.js, TypeScript, JavaScript, HTML/CSS | Node.js, Python (FastAPI/Django), REST/GraphQL |\n",
            "PostgreSQL, MongoDB, Redis | AWS (Lambda, EC2), GCP, Docker, Kubernetes, Jenkins | TensorFlow,\n",
            "Hugging Face, Scikit-learn, Apache Airflow |\n",
            "\n",
            "- **Other:** Git, Agile/Scrum, Terraform, WebSockets, Data Privacy (GDPR)\n",
            "\n",
            "## Education\n",
            "**Bachelor of Science in Computer Science**  \n",
            "University of California, Berkeley  \n",
            "*Graduated: May 2018*  \n",
            "- GPA: 3.8/4.0  \n",
            "- Relevant Coursework: Algorithms, Machine Learning, Distributed Systems  \n",
            "- Capstone Project: AI-Powered Chatbot for Customer Support (built with Python and React, presented\n",
            "at TechFest 2018)\n",
            "\n",
            "## Certifications & Projects\n",
            "- AWS Certified Developer ‚Äì Associate (2023)  \n",
            "- Google Cloud Professional Machine Learning Engineer (2022)  \n",
            "- **Personal Project:** Open-source AI Analytics Tool (GitHub: 500+ stars) ‚Äì A React-based dashboard\n",
            "integrating PyTorch models for real-time stock predictions.\n",
            "\n",
            "References available upon request.\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 8"
      ],
      "metadata": {
        "id": "7yK6K_J0tRqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Load and Display Job Description\n",
        "print(\"=\" * 100)\n",
        "print(\"üíº LOADING JOB DESCRIPTION\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Check for any file containing \"job_description\" or \"job\" in uploads folder\n",
        "job_file = None\n",
        "if os.path.exists('uploads'):\n",
        "    for filename in os.listdir('uploads'):\n",
        "        # Look for \"job_description\" or \"job\" (but not \"resume\")\n",
        "        if ('job_description' in filename.lower() or ('job' in filename.lower() and 'resume' not in filename.lower())) \\\n",
        "           and filename.endswith(('.txt', '.pdf', '.docx')):\n",
        "            job_file = os.path.join('uploads', filename)\n",
        "            break\n",
        "\n",
        "if not job_file:\n",
        "    print(\"\\n‚ùå ERROR: No job description file found!\")\n",
        "    print(\"Please upload a file containing 'job_description' or 'job' in the name:\")\n",
        "    print(\"  ‚Ä¢ uploads/job_description.txt\")\n",
        "    print(\"  ‚Ä¢ uploads/Senior_Developer_Job.pdf\")\n",
        "    print(\"  ‚Ä¢ uploads/software_engineer_job_posting.docx\")\n",
        "    print(\"  ‚Ä¢ etc.\")\n",
        "    raise FileNotFoundError(\"Job description file not found in uploads/ folder\")\n",
        "\n",
        "# Load job description text\n",
        "print(f\"\\n‚úÖ Found: {job_file}\")\n",
        "\n",
        "if job_file.endswith('.txt'):\n",
        "    with open(job_file, 'r', encoding='utf-8') as f:\n",
        "        job_text = f.read()\n",
        "elif job_file.endswith('.pdf'):\n",
        "    # For PDF support, would need PyPDF2 or similar\n",
        "    print(\"‚ö†Ô∏è PDF support not yet implemented - please use .txt for now\")\n",
        "    raise NotImplementedError(\"PDF parsing not implemented\")\n",
        "elif job_file.endswith('.docx'):\n",
        "    # For DOCX support, would need python-docx\n",
        "    print(\"‚ö†Ô∏è DOCX support not yet implemented - please use .txt for now\")\n",
        "    raise NotImplementedError(\"DOCX parsing not implemented\")\n",
        "\n",
        "print(f\"Length: {len(job_text):,} characters\")\n",
        "print(f\"Word count: {len(job_text.split())} words\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"JOB DESCRIPTION CONTENT:\")\n",
        "print(\"-\" * 100)\n",
        "print()\n",
        "print(format_text_readable(job_text, line_length=100))\n",
        "print()\n",
        "print(\"=\" * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dLrYEPtSck",
        "outputId": "52b9f5f4-409a-4ee7-bad6-57028e16b5ca"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "üíº LOADING JOB DESCRIPTION\n",
            "====================================================================================================\n",
            "\n",
            "‚úÖ Found: uploads/example_swe_job_description.txt\n",
            "Length: 3,481 characters\n",
            "Word count: 460 words\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "JOB DESCRIPTION CONTENT:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "# Job Description: Senior Full-Stack Software Engineer\n",
            "\n",
            "**Position:** Senior Full-Stack Software Engineer  \n",
            "**Company:** TechNova Innovations (a mid-sized SaaS company specializing in AI-driven analytics\n",
            "tools)\n",
            "**Location:** Remote (US-based preferred)  \n",
            "**Employment Type:** Full-time  \n",
            "**Salary Range:** $140,000 - $180,000 (depending on experience) + equity and benefits  \n",
            "**Posted Date:** November 4, 2025  \n",
            "\n",
            "## About TechNova Innovations\n",
            "TechNova is at the forefront of AI-powered data analytics, helping enterprises unlock insights from\n",
            "complex datasets. We're a collaborative team of 150+ engineers, designers, and product experts\n",
            "building scalable, user-centric platforms. Join us to shape the future of intelligent software\n",
            "solutions.\n",
            "\n",
            "## Job Summary\n",
            "We're seeking a seasoned Full-Stack Software Engineer to lead the development of our next-generation\n",
            "analytics dashboard. You'll collaborate with cross-functional teams to design, build, and optimize\n",
            "features that handle petabyte-scale data with real-time AI integrations. This role requires a blend\n",
            "of technical depth, problem-solving prowess, and a passion for clean, maintainable code.\n",
            "\n",
            "## Key Responsibilities\n",
            "- Architect and implement scalable backend services using Node.js, Python (FastAPI/Django), and\n",
            "cloud-native technologies (AWS/GCP).\n",
            "- Develop responsive front-end interfaces with React.js/TypeScript, ensuring seamless UX for data\n",
            "visualization libraries like D3.js or Chart.js.\n",
            "- Integrate AI/ML models (e.g., via TensorFlow or Hugging Face) into the platform for predictive\n",
            "analytics and anomaly detection.\n",
            "- Collaborate on CI/CD pipelines with tools like Jenkins, GitHub Actions, and Docker/Kubernetes for\n",
            "deployment.\n",
            "- Optimize database schemas and queries in PostgreSQL/MongoDB for high-performance data handling.\n",
            "- Conduct code reviews, mentor junior engineers, and drive agile methodologies in sprints.\n",
            "- Troubleshoot production issues, monitor system performance, and implement security best practices\n",
            "(e.g., OAuth, encryption).\n",
            "\n",
            "## Required Qualifications\n",
            "- Bachelor's or Master's in Computer Science, Software Engineering, or related field (or equivalent\n",
            "experience).\n",
            "- 5+ years of professional experience in full-stack development, with at least 2 years in a\n",
            "senior/lead role.\n",
            "- Proficiency in JavaScript/TypeScript, React.js, Node.js/Python, and SQL/NoSQL databases.\n",
            "- Hands-on experience with cloud platforms (AWS, GCP, or Azure) and containerization (Docker,\n",
            "Kubernetes).\n",
            "- Strong understanding of RESTful APIs, microservices, and event-driven architectures.\n",
            "- Familiarity with AI/ML frameworks and data pipelines (e.g., Apache Kafka, Airflow).\n",
            "- Excellent communication skills and ability to thrive in a fast-paced, remote environment.\n",
            "\n",
            "## Preferred Skills\n",
            "- Experience with GraphQL, WebSockets for real-time features.\n",
            "- Contributions to open-source projects or publications in tech blogs.\n",
            "- Knowledge of DevOps tools like Terraform for infrastructure as code.\n",
            "- Passion for ethical AI and data privacy (GDPR/HIPAA compliance).\n",
            "\n",
            "## What We Offer\n",
            "- Competitive salary, unlimited PTO, and comprehensive health benefits.\n",
            "- Professional development stipend for conferences and courses.\n",
            "- Flexible remote work with quarterly team offsites.\n",
            "- Ownership of impactful projects with room for innovation.\n",
            "\n",
            "**How to Apply:** Send your resume and a cover letter highlighting a challenging project you've led\n",
            "to careers@technova.io. We encourage diverse applicants and are an equal opportunity employer.\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 9"
      ],
      "metadata": {
        "id": "fZlrXl8GtWso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Run Inference Pipeline\n",
        "print(\"=\" * 100)\n",
        "print(\"üîÑ RUNNING MATCHING PIPELINE\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(\"\\n‚è≥ Processing... This may take 30-60 seconds...\\n\")\n",
        "\n",
        "# Run the complete matching pipeline\n",
        "print(\"1Ô∏è‚É£ Preprocessing texts...\")\n",
        "print(\"2Ô∏è‚É£ Extracting skills (rule-based & ML)...\")\n",
        "print(\"3Ô∏è‚É£ Calculating BERT semantic similarity...\")\n",
        "print(\"4Ô∏è‚É£ Calculating TF-IDF keyword similarity...\")\n",
        "print(\"5Ô∏è‚É£ Predicting job categories...\")\n",
        "print(\"6Ô∏è‚É£ Matching skills (fuzzy & semantic)...\")\n",
        "print(\"7Ô∏è‚É£ Computing final scores...\\n\")\n",
        "\n",
        "# Run inference\n",
        "result = match_resume_to_job_with_ml(resume_text, job_text)\n",
        "\n",
        "print(\"‚úÖ Inference complete!\")\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üìä Results ready! Run next cell to view detailed analysis.\")\n",
        "print(\"=\" * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctRJnIU0tXY9",
        "outputId": "88af28bf-abcb-4b85-edd9-0dedc6259586"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "üîÑ RUNNING MATCHING PIPELINE\n",
            "====================================================================================================\n",
            "\n",
            "‚è≥ Processing... This may take 30-60 seconds...\n",
            "\n",
            "1Ô∏è‚É£ Preprocessing texts...\n",
            "2Ô∏è‚É£ Extracting skills (rule-based & ML)...\n",
            "3Ô∏è‚É£ Calculating BERT semantic similarity...\n",
            "4Ô∏è‚É£ Calculating TF-IDF keyword similarity...\n",
            "5Ô∏è‚É£ Predicting job categories...\n",
            "6Ô∏è‚É£ Matching skills (fuzzy & semantic)...\n",
            "7Ô∏è‚É£ Computing final scores...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Inference complete!\n",
            "\n",
            "====================================================================================================\n",
            "üìä Results ready! Run next cell to view detailed analysis.\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 10"
      ],
      "metadata": {
        "id": "sl1GNse_tYzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Display Detailed Results\n",
        "print(\"=\" * 100)\n",
        "print(\"üìä RESUME-JOB MATCHING RESULTS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Extract components\n",
        "approach1 = result['approach_1_rule_based']\n",
        "approach2 = result['approach_2_ml_based']\n",
        "\n",
        "# ============================================================================\n",
        "# SHARED COMPONENTS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SHARED COMPONENTS\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"\\n  BERT Semantic Similarity:  {result['bert_similarity']:.1%} (40% weight)\")\n",
        "print(f\"  TF-IDF Keyword Similarity: {result['tfidf_similarity']:.1%} (25% weight)\")\n",
        "print(f\"  Category Match:            {result['category_match']:.1%} (15% weight)\")\n",
        "print(f\"\\n  Resume Category:    {result['resume_category']}\")\n",
        "print(f\"  Job Category:       {result['job_category']}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SKILLS EXTRACTED FROM RESUME\n",
        "# ============================================================================\n",
        "print(\"\\n\\n\" + \"=\" * 100)\n",
        "print(\"SKILLS EXTRACTED FROM RESUME\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nüìã Rule-Based (Fuzzy) - {len(approach1['resume_skills'])} skills:\")\n",
        "if len(approach1['resume_skills']) > 0:\n",
        "    print(f\"   {sorted(approach1['resume_skills'])}\")\n",
        "else:\n",
        "    print(\"   (none extracted)\")\n",
        "\n",
        "print(f\"\\nüß† ML-Based (Neural) - {len(approach2['resume_skills'])} skills:\")\n",
        "if len(approach2['resume_skills']) <= 30:\n",
        "    print(f\"   {sorted(approach2['resume_skills'])}\")\n",
        "else:\n",
        "    print(f\"   {sorted(approach2['resume_skills'])[:30]}\")\n",
        "    print(f\"   ... and {len(approach2['resume_skills']) - 30} more\")\n",
        "\n",
        "# ============================================================================\n",
        "# SKILLS EXTRACTED FROM JOB DESCRIPTION\n",
        "# ============================================================================\n",
        "print(\"\\n\\n\" + \"=\" * 100)\n",
        "print(\"SKILLS EXTRACTED FROM JOB DESCRIPTION\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nüìã Rule-Based (Fuzzy) - {len(approach1['job_skills'])} skills:\")\n",
        "if len(approach1['job_skills']) > 0:\n",
        "    print(f\"   {sorted(approach1['job_skills'])}\")\n",
        "else:\n",
        "    print(\"   (none extracted)\")\n",
        "\n",
        "print(f\"\\nüß† ML-Based (Neural) - {len(approach2['job_skills'])} skills:\")\n",
        "if len(approach2['job_skills']) <= 30:\n",
        "    print(f\"   {sorted(approach2['job_skills'])}\")\n",
        "else:\n",
        "    print(f\"   {sorted(approach2['job_skills'])[:30]}\")\n",
        "    print(f\"   ... and {len(approach2['job_skills']) - 30} more\")\n",
        "\n",
        "# ============================================================================\n",
        "# MATCHED SKILLS\n",
        "# ============================================================================\n",
        "print(\"\\n\\n\" + \"=\" * 100)\n",
        "print(\"MATCHED SKILLS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nüìã Rule-Based (Fuzzy) - {len(approach1['matched_skills'])} matches:\")\n",
        "if len(approach1['matched_skills']) > 0:\n",
        "    for match in approach1['matched_skills']:\n",
        "        print(f\"   ‚úì '{match['job_requires']}' ‚Üî '{match['resume_has']}' ({match['similarity']:.0f}% similar)\")\n",
        "else:\n",
        "    print(\"   (no matches found)\")\n",
        "\n",
        "print(f\"\\nüß† ML-Based (Neural) - {len(approach2['matched_skills'])} matches:\")\n",
        "if len(approach2['matched_skills']) > 0:\n",
        "    for match in approach2['matched_skills']:\n",
        "        print(f\"   ‚úì '{match['job_requires']}' ‚Üî '{match['resume_has']}' ({match['similarity']:.1f}% similar)\")\n",
        "else:\n",
        "    print(\"   (no matches found)\")\n",
        "\n",
        "# ============================================================================\n",
        "# MISSING SKILLS (GAP ANALYSIS)\n",
        "# ============================================================================\n",
        "print(\"\\n\\n\" + \"=\" * 100)\n",
        "print(\"MISSING SKILLS (GAP ANALYSIS)\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nüìã Rule-Based (Fuzzy) - {len(approach1['missing_skills'])} missing:\")\n",
        "if len(approach1['missing_skills']) > 0:\n",
        "    for skill in sorted(approach1['missing_skills']):\n",
        "        print(f\"   ‚úó {skill}\")\n",
        "else:\n",
        "    print(\"   ‚úì No missing skills!\")\n",
        "\n",
        "print(f\"\\nüß† ML-Based (Neural) - {len(approach2['missing_skills'])} missing:\")\n",
        "if len(approach2['missing_skills']) > 0:\n",
        "    # Show first 20, then indicate more\n",
        "    missing_sorted = sorted(approach2['missing_skills'])\n",
        "    if len(missing_sorted) <= 20:\n",
        "        for skill in missing_sorted:\n",
        "            print(f\"   ‚úó {skill}\")\n",
        "    else:\n",
        "        for skill in missing_sorted[:20]:\n",
        "            print(f\"   ‚úó {skill}\")\n",
        "        print(f\"   ... and {len(missing_sorted) - 20} more\")\n",
        "else:\n",
        "    print(\"   ‚úì No missing skills!\")\n",
        "\n",
        "# ============================================================================\n",
        "# SKILLS COMPONENT SCORE\n",
        "# ============================================================================\n",
        "print(\"\\n\\n\" + \"=\" * 100)\n",
        "print(\"SKILLS COMPONENT SCORE (20% weight)\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nüìã Rule-Based (Fuzzy):  {approach1['skill_component_score']:.1%}\")\n",
        "print(f\"   Match rate: {approach1['match_rate']:.1%}\")\n",
        "print(f\"   ({len(approach1['matched_skills'])}/{len(approach1['job_skills'])} skills matched)\")\n",
        "\n",
        "print(f\"\\nüß† ML-Based (Neural):   {approach2['skill_component_score']:.1%}\")\n",
        "print(f\"   Match rate: {approach2['match_rate']:.1%}\")\n",
        "print(f\"   ({len(approach2['matched_skills'])}/{len(approach2['job_skills'])} skills matched)\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SCORES\n",
        "# ============================================================================\n",
        "print(\"\\n\\n\" + \"=\" * 100)\n",
        "print(\"FINAL SCORES\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nüìã Rule-Based (Fuzzy):  {approach1['final_score']:.1%}\")\n",
        "print(f\"   Recommendation: {approach1['recommendation']}\")\n",
        "\n",
        "print(f\"\\nüß† ML-Based (Neural):   {approach2['final_score']:.1%}\")\n",
        "print(f\"   Recommendation: {approach2['recommendation']}\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\\n\" + \"=\" * 100)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\n{'Metric':<40} {'Rule-Based':<15} {'ML-Based':<15} {'Difference'}\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"{'Final Score':<40} {approach1['final_score']:.1%}{'':>8} {approach2['final_score']:.1%}{'':>8} {(approach2['final_score']-approach1['final_score']):+.1%}\")\n",
        "print(f\"{'Skills Component (20%)':<40} {approach1['skill_component_score']:.1%}{'':>8} {approach2['skill_component_score']:.1%}{'':>8} {(approach2['skill_component_score']-approach1['skill_component_score']):+.1%}\")\n",
        "print(f\"{'Resume Skills Extracted':<40} {len(approach1['resume_skills']):<15} {len(approach2['resume_skills']):<15} {len(approach2['resume_skills'])-len(approach1['resume_skills']):+d}\")\n",
        "print(f\"{'Job Skills Extracted':<40} {len(approach1['job_skills']):<15} {len(approach2['job_skills']):<15} {len(approach2['job_skills'])-len(approach1['job_skills']):+d}\")\n",
        "print(f\"{'Skills Matched':<40} {len(approach1['matched_skills']):<15} {len(approach2['matched_skills']):<15} {len(approach2['matched_skills'])-len(approach1['matched_skills']):+d}\")\n",
        "print(f\"{'Match Rate':<40} {approach1['match_rate']:.1%}{'':>8} {approach2['match_rate']:.1%}{'':>8} {(approach2['match_rate']-approach1['match_rate']):+.1%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"‚úÖ ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSHX4e-xtZnF",
        "outputId": "a75e7b99-3db1-4d59-ebb2-494bc5c946e8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "üìä RESUME-JOB MATCHING RESULTS\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "SHARED COMPONENTS\n",
            "====================================================================================================\n",
            "\n",
            "  BERT Semantic Similarity:  96.5% (40% weight)\n",
            "  TF-IDF Keyword Similarity: 54.8% (25% weight)\n",
            "  Category Match:            100.0% (15% weight)\n",
            "\n",
            "  Resume Category:    Java Developer\n",
            "  Job Category:       Java Developer\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "SKILLS EXTRACTED FROM RESUME\n",
            "====================================================================================================\n",
            "\n",
            "üìã Rule-Based (Fuzzy) - 21 skills:\n",
            "   ['aws', 'distributed systems', 'django', 'docker', 'gcp', 'git', 'google cloud', 'javascript', 'jenkins', 'kubernetes', 'machine learning', 'mongodb', 'postgresql', 'python', 'pytorch', 'react', 'redis', 'scikit-learn', 'software engineer', 'tensorflow', 'terraform']\n",
            "\n",
            "üß† ML-Based (Neural) - 72 skills:\n",
            "   ['- dev | github.', 'agile / scrum', 'agile methodologies', 'ai / ml', 'ai ml', 'algorithms', 'apache airflow', 'api', 'aws', 'aws lambda', 'bachelor of science', 'backend', 'best practices', 'capstone', 'cd pipelines', 'ci', 'ci / cd pipelines', 'cloud', 'cloud & devops *', 'cloud architectures', 'computer science', 'd3. js', 'data privacy', 'databases & tools', 'distributed systems', 'docker', 'ec2', 'email', 'end', 'event']\n",
            "   ... and 42 more\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "SKILLS EXTRACTED FROM JOB DESCRIPTION\n",
            "====================================================================================================\n",
            "\n",
            "üìã Rule-Based (Fuzzy) - 17 skills:\n",
            "   ['aws', 'azure', 'django', 'docker', 'gcp', 'javascript', 'jenkins', 'kubernetes', 'mongodb', 'postgresql', 'python', 'react', 'software engineer', 'software engineering', 'sql', 'tensorflow', 'terraform']\n",
            "\n",
            "üß† ML-Based (Neural) - 60 skills:\n",
            "   ['# # preferred', 'ai', 'ai / ml', 'airflow', 'anomaly detection', 'apache kafka', 'aws', 'aws / gcp', 'aws / gcp )', 'azure', 'backend', 'chart. js', 'ci / cd pipelines', 'cloud -', 'cloud platforms', 'collaborate', 'containerization', 'd3. js', 'data handling', 'data pipelines', 'data privacy', 'data visualization', 'devops tools', 'django', 'docker', 'docker / kubernetes', 'encryption', 'end', 'fast', 'fastapi / django']\n",
            "   ... and 30 more\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "MATCHED SKILLS\n",
            "====================================================================================================\n",
            "\n",
            "üìã Rule-Based (Fuzzy) - 15 matches:\n",
            "   ‚úì 'software engineer' ‚Üî 'software engineer' (100% similar)\n",
            "   ‚úì 'tensorflow' ‚Üî 'tensorflow' (100% similar)\n",
            "   ‚úì 'mongodb' ‚Üî 'mongodb' (100% similar)\n",
            "   ‚úì 'gcp' ‚Üî 'gcp' (100% similar)\n",
            "   ‚úì 'javascript' ‚Üî 'javascript' (100% similar)\n",
            "   ‚úì 'python' ‚Üî 'python' (100% similar)\n",
            "   ‚úì 'docker' ‚Üî 'docker' (100% similar)\n",
            "   ‚úì 'software engineering' ‚Üî 'software engineer' (92% similar)\n",
            "   ‚úì 'jenkins' ‚Üî 'jenkins' (100% similar)\n",
            "   ‚úì 'django' ‚Üî 'django' (100% similar)\n",
            "   ‚úì 'aws' ‚Üî 'aws' (100% similar)\n",
            "   ‚úì 'react' ‚Üî 'react' (100% similar)\n",
            "   ‚úì 'terraform' ‚Üî 'terraform' (100% similar)\n",
            "   ‚úì 'postgresql' ‚Üî 'postgresql' (100% similar)\n",
            "   ‚úì 'kubernetes' ‚Üî 'kubernetes' (100% similar)\n",
            "\n",
            "üß† ML-Based (Neural) - 40 matches:\n",
            "   ‚úì 'tensorflow' ‚Üî 'tensorflow' (100.0% similar)\n",
            "   ‚úì 'github actions' ‚Üî 'github actions' (100.0% similar)\n",
            "   ‚úì 'cloud platforms' ‚Üî 'cloud' (74.9% similar)\n",
            "   ‚úì 'airflow' ‚Üî 'apache airflow' (76.0% similar)\n",
            "   ‚úì 'jenkins' ‚Üî 'jenkins' (100.0% similar)\n",
            "   ‚úì 'django' ‚Üî 'fastapi / django' (64.9% similar)\n",
            "   ‚úì 'websockets' ‚Üî 'websockets' (100.0% similar)\n",
            "   ‚úì 'full' ‚Üî 'full' (100.0% similar)\n",
            "   ‚úì 'sql / nosql databases' ‚Üî 'databases & tools' (68.0% similar)\n",
            "   ‚úì 'kubernetes' ‚Üî 'kubernetes' (100.0% similar)\n",
            "   ‚úì 'data privacy' ‚Üî 'data privacy' (100.0% similar)\n",
            "   ‚úì 'd3. js' ‚Üî 'd3. js' (100.0% similar)\n",
            "   ‚úì 'backend' ‚Üî 'backend' (100.0% similar)\n",
            "   ‚úì 'software engineering' ‚Üî 'computer science' (65.3% similar)\n",
            "   ‚úì 'aws' ‚Üî 'aws' (100.0% similar)\n",
            "   ‚úì 'aws / gcp' ‚Üî 'gcp' (74.2% similar)\n",
            "   ‚úì 'react. js / typescript' ‚Üî 'react. js / typescript' (100.0% similar)\n",
            "   ‚úì 'postgresql / mongodb' ‚Üî 'mongodb' (77.4% similar)\n",
            "   ‚úì 'javascript typescript' ‚Üî 'typescript' (91.0% similar)\n",
            "   ‚úì 'restful apis' ‚Üî 'restful apis' (100.0% similar)\n",
            "   ‚úì 'end' ‚Üî 'end' (100.0% similar)\n",
            "   ‚úì 'fastapi / django' ‚Üî 'fastapi / django' (100.0% similar)\n",
            "   ‚úì 'ml' ‚Üî 'ml' (100.0% similar)\n",
            "   ‚úì 'docker' ‚Üî 'docker' (100.0% similar)\n",
            "   ‚úì 'devops tools' ‚Üî 'cloud & devops *' (69.2% similar)\n",
            "   ‚úì 'kubernetes )' ‚Üî 'kubernetes' (95.3% similar)\n",
            "   ‚úì 'node. js' ‚Üî 'node. js / python' (77.3% similar)\n",
            "   ‚úì 'aws / gcp )' ‚Üî 'gcp' (72.6% similar)\n",
            "   ‚úì 'hugging face' ‚Üî 'hugging face' (100.0% similar)\n",
            "   ‚úì 'ai' ‚Üî 'ai ml' (75.0% similar)\n",
            "   ‚úì 'gcp' ‚Üî 'gcp' (100.0% similar)\n",
            "   ‚úì 'node. js / python' ‚Üî 'node. js / python' (100.0% similar)\n",
            "   ‚úì 'docker / kubernetes' ‚Üî 'kubernetes' (77.8% similar)\n",
            "   ‚úì 'python' ‚Üî 'python' (100.0% similar)\n",
            "   ‚úì 'cloud -' ‚Üî 'cloud' (93.7% similar)\n",
            "   ‚úì 'ai / ml' ‚Üî 'ai / ml' (100.0% similar)\n",
            "   ‚úì 'react. js' ‚Üî 'react. js' (100.0% similar)\n",
            "   ‚úì 'data pipelines' ‚Üî 'cd pipelines' (63.3% similar)\n",
            "   ‚úì 'terraform' ‚Üî 'terraform' (100.0% similar)\n",
            "   ‚úì 'ci / cd pipelines' ‚Üî 'ci / cd pipelines' (100.0% similar)\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "MISSING SKILLS (GAP ANALYSIS)\n",
            "====================================================================================================\n",
            "\n",
            "üìã Rule-Based (Fuzzy) - 2 missing:\n",
            "   ‚úó azure\n",
            "   ‚úó sql\n",
            "\n",
            "üß† ML-Based (Neural) - 20 missing:\n",
            "   ‚úó # # preferred\n",
            "   ‚úó anomaly detection\n",
            "   ‚úó apache kafka\n",
            "   ‚úó azure\n",
            "   ‚úó chart. js\n",
            "   ‚úó collaborate\n",
            "   ‚úó containerization\n",
            "   ‚úó data handling\n",
            "   ‚úó data visualization\n",
            "   ‚úó encryption\n",
            "   ‚úó fast\n",
            "   ‚úó gdpr / hipaa compliance\n",
            "   ‚úó graphql\n",
            "   ‚úó infrastructure as code\n",
            "   ‚úó microservices\n",
            "   ‚úó oauth\n",
            "   ‚úó remote\n",
            "   ‚úó responsive front -\n",
            "   ‚úó tech\n",
            "   ‚úó thrive\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "SKILLS COMPONENT SCORE (20% weight)\n",
            "====================================================================================================\n",
            "\n",
            "üìã Rule-Based (Fuzzy):  91.6%\n",
            "   Match rate: 88.2%\n",
            "   (15/17 skills matched)\n",
            "\n",
            "üß† ML-Based (Neural):   73.8%\n",
            "   Match rate: 66.7%\n",
            "   (40/60 skills matched)\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "FINAL SCORES\n",
            "====================================================================================================\n",
            "\n",
            "üìã Rule-Based (Fuzzy):  85.6%\n",
            "   Recommendation: Strong Match! üéØ\n",
            "\n",
            "üß† ML-Based (Neural):   82.0%\n",
            "   Recommendation: Strong Match! üéØ\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "COMPARISON SUMMARY\n",
            "====================================================================================================\n",
            "\n",
            "Metric                                   Rule-Based      ML-Based        Difference\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Final Score                              85.6%         82.0%         -3.6%\n",
            "Skills Component (20%)                   91.6%         73.8%         -17.8%\n",
            "Resume Skills Extracted                  21              72              +51\n",
            "Job Skills Extracted                     17              60              +43\n",
            "Skills Matched                           15              40              +25\n",
            "Match Rate                               88.2%         66.7%         -21.6%\n",
            "\n",
            "====================================================================================================\n",
            "‚úÖ ANALYSIS COMPLETE\n",
            "====================================================================================================\n"
          ]
        }
      ]
    }
  ]
}